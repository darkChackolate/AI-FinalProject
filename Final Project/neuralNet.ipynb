{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different file, will merge eventually\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/jzlyn/OneDrive/Desktop/CS440/AI-FinalProject/Final Project/Berkeley_Example')\n",
    "# sys.path.insert(1, 'C:/Users/jzlyn/OneDrive/Desktop/CS419/AI/AI-FinalProject/Final Project/Berkeley_Example') home pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    try:\n",
    "        sig =1/(1 + np.exp(-x))\n",
    "    except:\n",
    "        sig = 0   \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training\ta\tNeural\tNetwork\t\n",
    "1. Randomly\tinitialize\tweights\t\n",
    "2. Implement\tforward\tpropaga1on\tto\tget\th_Θ(x_i) for any instance x_i\n",
    "3. Implement\tcode\tto\tcompute\tcost\tfunc1on\tJ(Θ)\n",
    "4. Implement\tbackprop\tto\tcompute\tpar1al\tderiva1ves\t\n",
    "5. Use\tgradient\tchecking\tto\tcompare\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "computed\tusing\tbackpropaga1on\tvs.\tthe\tnumerical\t\n",
    "gradient\tes1mate.\t\t\t\n",
    "- Then,\tdisable\tgradient\tchecking\tcode\t\n",
    "6. Use\tgradient\tdescent\twith\tbackprop\tto\tfit\tthe\tnetwork\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  \n",
    "# Number of neurons in the hidden layer\n",
    "hidden_size = 128  \n",
    "# Number of output classes (10 for digits 0-9)\n",
    "output_size = 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. randomly initialize weights\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "b2 = np.zeros(output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement forward propagation to get h_Θ(x_i) for any instance x_i\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # X is the input data\n",
    "    # W1, b1 are weights and biases from input layer to hidden layer\n",
    "    # W2, b2 are weights and biases from hidden layer to output layer\n",
    "    \n",
    "    #Input to hidden layer\n",
    "    Z1 = np.dot(W1, X) + b1  \n",
    "    A1 = sigmoid(Z1)         \n",
    "    # Step 2: Hidden layer to output layer\n",
    "    Z2 = np.dot(W2, A1) + b2  \n",
    "    A2 = softmax(Z2)          \n",
    "\n",
    "    return A2  # The output of the network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Implement code to compute cost function J(Θ)\n",
    "def compute_cost(Y, predictions):\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "    cost = -(1/m) * np.sum(Y * np.log(predictions) + (1 - Y) * np.log(1 - predictions))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Implement backprop to compute partial derivatives\n",
    "def backward_propagation(X, Y, W1, b1, W2, b2, A1, A2):\n",
    "    m = Y.shape[1]  # Number of examples\n",
    "    \n",
    "    # Calculate dZ2 at the output layer\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate dZ1 at the hidden layer\n",
    "    dZ1 = np.dot(W2.T, dZ2) * sigmoid_derivative(A1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "digit_train_imgs = samples.loadDataFile('data/digitdata/trainingimages', n, 28, 28)\n",
    "digit_train_labs = samples.loadLabelsFile('data/digitdata/traininglabels', n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
